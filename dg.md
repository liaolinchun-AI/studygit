#大纲
##1.自然语言处理与深度学习简介

- 什么是自然语言处理(NLP)?
  - 自然语言处理是一门计算机学科、人工智能以及语言学的交叉学科。
  - 目标：让计算机处理或“理解”自然语言
  - 完全理解和表达语言极其困难
  - 完美的语言理解等同于实现人工智能
- NLP的研究内容：
  - 信息检索
  - 机器翻译
  - 文档分类
  - 问答系统
  - 信息过滤
  - 自动文摘
  - 信息抽取
  - 文本挖掘
  - 舆情分析
  - 机器写作
  - 文稿机器校队
  - 语音识别
- 自然语言处理应用
  - 拼写检查，关键词检索，查找同义词
  - 文本挖掘
    - 产品价格、日期、时间、地点、人名、公司名
  - 文本分类：文档分类，情感分析
  - 客服系统
  - 复杂对话系统
- NLP在工业中的应用
  - 搜索(书面及口头)
  - 在线广告匹配
  - 自动/辅助翻译
  - 市场或财务/贸易的情感舆情分析
  - 语音识别
  - 聊天机器人/对话代理
- 人类语言有什么特别之处
- NLP的技术发展
  - 过去：以语言学为主要基础的时代
  - 现在：以统计方法为主流的时代
  - 将来：深度学习等人工智能最新技术在NLP领域中的应用
- 深度学习
  - 深度学习是机器学习的一个子集
  - 表示学习试图自动学习特征的表现
  - 自动特征学习比手工特征快、方便扩展。
  - 深度学习即可以无监督学习，也可以监督学习
- 深度学习之前为什么没有，后面才崛起呢？(深度学习崛起：先是语音与图像，后来才是NLP)
  - 以前没有大量的训练数据支持深度学习，现在有
  - 以前没有更快的机器和多核CPU/GPU支持深度学习，现在有
  - 以前没有关于深度学习的模型、算法、想法，现在有
- 为什么NLP难？
  - 场景的困难：语言多样性，上下文(语境)、多变性，歧义性
  - 学习的困难：艰难的数学模型(概率图模型：HMM，最大熵，CRF等)

##2.语言模型
##3.文本表示

- 文本分类
  - 文本表示分类(基于粒度)
    - 长文本表示
    - 短文本表示(句子)
    - 词表示
  - 文本表示分类(基于表示方法)
    - 离散表示
      - One-hot表示
      - BOW(Bag of Words)
        - 优点：
          - 简单、方便、快捷
          - 在语料充足的前提下，对于简单的自然语言处理任务效果不错
        - 缺点：
          - 准确率比较低，凡是出现在文本中的词一视同仁，不能体现不同词在一句话中的不同的重要性。
          - 无法关注词语之间的顺序关系，这是词袋子模型最大的缺点
      - TF-IDF
        - TF(词频)     一个单词在整篇文章中的占比
        - IDF(逆文档频率)   文档总数/(1+含有单词的文档数)
        - tfidf = TF*IDF
      - Bi-gram和N-gram
        - 理论上N越大越好，但词表膨胀导致计算量大，经验上是2和3
        - 优点：考虑了词的顺序
        - 缺点：词表的膨胀
      - Multi-hot表示
      - 离散表示的问题：
        - 无法衡量词向量之间的关系
        - 词表维度随着语料库增长而膨胀
        - n-gram词序列随语料库膨胀更快
        - 数据稀疏问题
    - 分布式表示
      - 基于矩阵表示
        - 基于降维表示
        - 基于聚类表示
      - 基于神经网络
        - NNLM
        - Word2vec
          - CBOW
            - 层次Softmax(使用Huffman Tree来编码输出层的词典)
            - 负例采样(一个正样本，n-1个负样本，对负样本做采样)
          - Skip-gram
          - Word2Vec存在的问题：
            - 对每个local context window单独训练，没有利用包含在global co-currence矩阵中的统计信息
            - 对多义词无法很好的表示和处理，因为使用了唯一的词向量(解决:Sense2Vec)
      - 共现矩阵作为词向量存在的问题：
        - 向量维数随着词典大小线性增长
        - 存储整个词典的空间消耗非常大
        - 一些模型如文本分类模型会面临稀疏性问题
        - 模型会欠稳定
        - 解决：
          - 可以进行SVD降维(但会存在问题)
            - 计算量大
            - 难为词典中新加入的词分配词向量
            - 与其他深度学习模型框架差异大

##4.文本相识度处理和文本匹配
##5.文本分类

- 传统文本分类方法
  - 特征工程
    - 文本预处理(提取关键字表示文本的过程)
      - 中文文本处理(文本分词和去停用词)
    - 文本特征提取
      - TF词频
      - TFIDF词频逆文档
      - Doc2vec
      - Word2vec
    - 文本表示
      - 把文本预处理后的转化成计算机可以理解的方式
  - 分类器
    - 朴素贝叶斯、KNN、SVM、最大熵和神经网络.....
  - 过程：训练集—>文本预处理—>特征提取—>文本表示—>分类器(顺序不知道对不对)
- 深度学习文本分类方法
  - 文本的分布式表示：词向量(word embedding)
  - 深度学习文本分类模型
    - FastText
    - TextCNN
    - TextRNN
    - TextRNN + Attention
    - TextRCNN(TextRNN + CNN)

##6.Seq2Seq模型和Attention Mechanism

- Seq2Seq以编码(Encode)和解码(Decode为代表的框架方式)，seq2seq模型是根据输入序列X来生成输出序列Y，在翻译，文本自动摘要和机器人自动问答以及一些回归预测任务上有着广泛的运用
- attention概念：所谓注意力机制可以粗略的理解为是一种对于输入的数据，根据重要程度进行不同权重处理(通常加权的权重来源于softmax后的结果)的机制，是一种在编码阶段，简单地对编码器中的hidden states 进行不同权重的加权处理的过程。
- 问题：由于encoder-decoder模型在编码和解码阶段始终有一个不变的语义向量C来联系着，编码器要将整个序列的信息压缩进一个固定长度的向量中去。这就造成了
  - 语义向量无法完全表示整个序列的信息
  - 最开始输入的序列容易被后输入的序列给覆盖掉，会丢失许多细节信息。在长序列上表现的尤为明显。
  - attention模型的引入使模型不再要求编码器将所有输入信息都编码进一个固定长度的向量中，而且根据重要程度进行不同的权重处理。
- beam Search(集束搜索)
  - 应用场景：机器翻译，语言识别，当系统的数据集比较大，计算资源受限，而且没有唯一最优解时，该算法能够较快的找到最接近的解。

##7.文本生成(主要模型Seq2Seq和attention机制以及其变种)

- 文本生成任务
  - 文本到文本的生成
    - 文本摘要
      - 抽取式摘要
      - 生成式摘要
      - 使用主题模型PLSA将句子按照主题进行聚类，使用SVR计算句子的相识度，最后使用线性规划生成相关工作文本
    - 古诗生成
      - 使用循环神经网络进行生成，将古诗生成划分为规划模型和生成模型两部份
    - 文本复述等
  - 数据到文本的生成(分为了信号处理、数据分析、文档规划、文本实现四个步骤)
    - 机器写作
  - 图像到文本的生成
- 文本生成方法
  - 基于语言模型的文本生成
  - 基于深度学习的文本生成
- 模型评价
  - 自动化评价
  - 人工评价

##8.分词、词性标注、命名实体识别
##9.信息提取（关键字提取、自动文摘、主题识别、关系抽取）

- 关键词提取

  - 从方法来说
    - 关键词分配：就是有一个给定的关键字词库，然后新来一篇文档，从词库里面找出几个词语作为这边文档的关键字
    - 关键词提取：就是新来一篇文档，从文档中抽取一些词语作为这篇文档的关键字
  - 从算法角度来说
    - 有监督学习算法：将关键词抽取过程视为二分类问题，先抽取出候选词，然后对于每个候选词划定标签，要么是关键词，要么不是关键词，然后训练关键词抽取分类器。当新来一篇文档时，抽取所有的候选词，然后利用训练好的关键词抽取分类器，对各个候选词进行分类，最终将标签为关键词的候选词作为关键词
    - 无监督学习算法，先抽取出候选词，然后对各个候选词进行打分，然后输出topK个分值最高的候选词作为关键词。根据打分的策略不同，有不同的算法。
      - TF-IDF
        - 优点：简单快速，容易理解。
        - 缺点：有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息(无序)，无法体现词在上下文的重要性。一种解决方法是：对全文的第一段和每一段的第一句话，给予较大的权重。另一种解决方法是：使用Word2vec算法来支持。
      - TextRank
        - 由PageRank改进而来
        - 思想：将文本中的语法单元(可以是词或者句子)视作图中的节点，如果两个语法单元存在一定语法关系(例如共现)，则这两个语法单元在图中就会有一条边相互连接，通过一定的迭代次数，最终不同的节点会有不同的权重，权重高的语法单元可以作为关键词(句子)。
        - 可以看到，节点的权重不仅依赖于它的入度节点,还依赖于这些入度节点的权重，入度节点越多，入度节点的权重越大，说明这个节点的权重越高。(入度：入度是图论算法中重要的概念之一。它通常指有向图中某点作为图中边的终点的次数之和。)
        - 可以应用到摘要生成

- 自动文摘

  - 第一种分类方法

    - extractive(抽取式)

      - 目前最主流、应用最多、最容易的方法

      - 假设文章由一句或几句概括，找出前几句话重要的句子(排序:考虑相关性和新颖性两个指标，相关性是指摘要所用的句子最能狗代表本文档的意思，而新颖性是指候选句子包含的冗余信息要少，尽可能每句话都可以独立地表达出一种独立的意思)

      - 步骤

        - 预处理：以句点或逗号或其他符号分割
        - 词、句表示：转化为计算机能理解的量
        - 排序：
          - 基于图排序(TextRank)
          - 基于特征(句子长度、句子位置、句子是否包含标题词、句子关键词打分)代表算法：TextTeaser
        - 后处理
          - 排序之后的结果只考虑了相关性并没有考虑新颖性，非常有可能出现排名靠前的几句话表达的都是相似的意思。所以需要引入一个惩罚因子，将新颖性考虑进去。对所有句子重新打分，通过相关性排序好的列表，再从第二句开始，后面的句子必须被和前一句的相似度进行惩罚。(MMR算法)
        - 输出
          - 取排序后的前N句话，这里涉及到一个非常重要的问题，也是一直自动文摘质量被诟病的问题，可读性：因为个个句子都是从不同的段落中选择出来的，如果只是生硬地连起来生成摘要的话，很难保证句子之间的衔接和连贯。保证可读性是一件很难的事情。

        

    - abstractive(抽象式)

      - 要求系统理解文挡所表达的意思，然后用可读性强的人类语言将其简练地总结出来。更有一种真正人工智能的味道
      - 难点：
        - 理解文挡
        - 可读性强
        - 简练总结

  - 第二种分类方法

    - 单文档摘要和多文档摘要，前者是后者的基础，但后者不只是前者结果简单叠加那么简单。

  - Evaluation(评价)

    - 分类
      - 人工评价
      - 自动评价
    - 重要性

- 主题提取

- 文本信息提取

##10.句法分析
##11.Transformer模型和Bert原理及应用
##12.综合项目

 -	12-1 [知识图谱]Knowledge_graph
   	-	实战1 - 农业知识图谱
   	-	实战2 - 医疗知识图谱
-	12-2 [智能聊天机器人] - Chatbot
  -	实战1 - Seq2Seq聊天机器人
  -	实战2 - 闲聊 + 任务型聊天机器人